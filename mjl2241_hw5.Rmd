---
title: "mjl2241_hw5"
author: "Michelle Lee"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document
---
This is my solution to HW5. 

```{r setup, include=FALSE}
library(tidyverse)
library (patchwork)
library(readr)
library(broom)
library(viridis)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```
### Problem 1.

We start off by downloading the data from The Washing Post, on on homicides in 50 large U.S. cities via Github. After taking a look at the dataset, we also created city_state variable and resolved_status variable to count the total number of homicides and unsolved homicides. 
```{r, error = TRUE}
homicide_df =
  tibble(
  read.csv(url("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"))) %>% 
  janitor::clean_names() 
skimr::skim(homicide_df)
  
homicide_df$city_state= 
  paste(homicide_df$city,",", homicide_df$state)

#create resolved variable;
homicide_df = 
  
  homicide_df %>%
  mutate(resolved = case_when(
    disposition == 'Closed without arrest' ~   "unsolved",
    disposition == 'Open/No arrest' ~ "unsolved",
    disposition == 'Closed by arrest' ~ "solved"))   %>%
  select(city_state, resolved)%>%
  filter(city_state != "Tulsa , AL")

homicide_status_df = 
  homicide_df %>%
  group_by(city_state) %>%
  summarize(
    total_hom = n(),
    unsolved_hom = sum(resolved == "unsolved")
  )
```
In this dataset, there is a total of 52,179 unique rows 12 columns. Some of the key variables are disposition, city, state, gender, victims age, victims race, victims name, and the date of the incident. The dataset is mostly complete and do not have missing values in key variables, except for 60 rows in latitude and longitude. 

Using the prop.test on Baltimore, MA
``` {r baltimore data, error = TRUE}
prop.test(
  homicide_status_df %>% filter(city_state == "Baltimore , MD") %>% pull(unsolved_hom), 
  homicide_status_df %>% filter(city_state == "Baltimore , MD") %>% pull(total_hom)) %>% 
  broom::tidy()
```
Applying prop.test on all the cities

``` {r prop.test on cities, error = TRUE}
results_df = 
  homicide_status_df %>% 
  mutate(
    prop_tests = map2(.x = unsolved_hom, .y = total_hom, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low,conf.high)
```
Then we finally created a plot that shows the estimates and CIs for each city.

``` {r plot for results_df, error = TRUE}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high))
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

### problem 2

``` {r read and tidy data, error = TRUE}
files = list.files("./data", pattern = ".csv", all.files = FALSE, 
full.names = FALSE)

study_df =
  tibble(
  data.frame(participants = files) %>% 
  mutate(file_contents = map(participants, ~read.csv(file.path("./data", .)))) %>% 
  separate(participants, into = c("arm", "subject_id")) %>% 
  unnest(file_contents) %>% 
  mutate(
    arm = recode(arm, `con` = "control", `exp` = "experiment")
  )) %>%
  select(arm, subject_id,week_1:week_8)%>%
  pivot_longer(
    week_1:week_8,
    names_to = "week_number",
    values_to = "value")
```

```{r, error = TRUE}
study_df = 
  tibble(
    filename = list.files("./data")
  ) %>% 
  mutate(
    path = str_c("./data/", filename),
    data = map(path, read_csv )
  )%>%
  unnest(data)%>%
  separate(filename, into = c("arm","subject_id","csv"), convert = TRUE )%>%
  select(arm, subject_id,week_1:week_8)%>%
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "value")%>%
  mutate(
    
    subject_id = as.character(subject_id)
  )
```
we then made a spaghetti plot

``` {r spaghetti plot, error = TRUE }
study_df %>% 
  ggplot(aes(x = week, y = value, group = subject_id, color = subject_id)) +
  geom_line() + 
  theme_bw () +
  facet_grid(~arm) +
  labs(
    title = "value over time by groups",
    x = "Week",
    y = "Value"
  ) + 
  viridis::scale_color_viridis(discrete = TRUE)
```
Based on the graphs above, participants in the experiment arm had increasing positive values as the week progress, on average. However, participants from the control arm had values that did not significantly increase as the weeks progressed and showed irregular change over time.

### Problem 3
First, we created the sim_regression function
```{r message = FALSE, warning = FALSE}
set.seed(1)

sim_ttest = function(n = 30, mu, sigma = 5) {
  sim_data = tibble (
  x = rnorm(n, mean = mu, sd = sigma)
  )
  
  sim_data %>%
  t.test() %>%
  broom::tidy() %>% 
  select(estimate, p.value)
}
```
We then ran the simulation 5000 times for μ={1,2,3,4,5,6} and saved to sim_results dataframe

``` {r sim_results df}
sim_results = tibble(true_mu = c(0,1,2,3,4,5,6)) %>% 
  mutate(
    output_list = map(.x = true_mu, ~rerun(5000, sim_ttest(mu = .x))),
    output_df = map(output_list, bind_rows)) %>% 
  select(-output_list) %>% 
  unnest(output_df)

head(sim_results)
```
Now let's make a plot showing the proportion of times the null was rejected. 
```{r plot of null_reject}
sim_results_reject_null =
  sim_results %>% 
    mutate(reject = case_when(
      p.value < 0.05 ~ 1,
      p.value >= 0.05 ~ 0))%>%  
  group_by(true_mu) 
sim_results_reject_null %>%
  summarise(power = mean(reject)) %>%  #proportion of time null is rejected
  ggplot(aes(x = true_mu, y = power)) +
  geom_point(alpha = .5) +
  geom_line(alpha =.5, color = "coral") +
  labs(title = "Power vs. Effect Size", x = "True value of mu", y="power")
  
```
Based on the graph, we can see that as the true value of mu increases, power increases as well. This indicates that rejecting the null increases as the effect size increases. 

Now, let's make another plot (graph 1) showing the average estimate of mu on the y axis and the true value of μ on the x axis. And make another graph (graph 2) showing the average estimate of μ̂ only in samples for which the null was rejected on the y axis and the true value of μ on the x axis. 

```{r average estimate of mu vs. true mu}
plot_1 = 
  sim_results_reject_null %>%
summarize(mean_mu = mean(estimate))%>%
  ggplot(aes(x = true_mu, y = mean_mu))+
  geom_point()+
  geom_line(alpha =.5, color = "coral")+
  labs(title = "Mean Estimate vs True Mu", x = "True value of Mu", y = "Mean Estimate")

plot_2 =
  sim_results_reject_null %>%
  filter(reject == 1) %>%
  summarize(mean_mu = mean(estimate))%>%
  ggplot(aes(x = true_mu, y = mean_mu))+
  geom_point()+
  geom_line(alpha =.5, color = "turquoise")+
  labs(title = "Mean Estimate_rejected H0 vs. True Mu", x = "True value of Mu", y = "Mean Estimate of reject null")

plot_1 + plot_2
```
In graph 2, the sample mean estimate of mu where null was rejected is not approximately equal to the true value of mu, because as the power decreases the mean estimates get further away from the true mean when the power increases. 